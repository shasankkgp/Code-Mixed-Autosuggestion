{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot mT5 Baseline Evaluation\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This notebook documents the initial phase of the project on \"Code-Mixed Query Auto-Suggestion.\" The primary objective of this phase is to establish a performance baseline using a pre-trained multilingual model, mT5. By running the model on code-mixed queries in a zero-shot setting (without any fine-tuning), we aim to demonstrate that a general-purpose model is insufficient for this specialized task. The poor results from this evaluation will provide a strong justification for the project's next steps: generating a high-quality synthetic corpus and fine-tuning a specialized model.\n",
        "\n"
      ],
      "metadata": {
        "id": "P31tc9NkTtZw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYHEEZE7OZc8",
        "outputId": "fe9f71bc-fa38-487e-8ede-9ecabdd71fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Zero-Shot for Hindi-English ---\n",
            "Input:    'mujhe aaj bahar jana hai, can you please tell me the'\n",
            "Output:   '<extra_id_0> aaj bahar jana.'\n",
            "\n",
            "Input:    'kya aap mujhe bata sakte hain, what is the nearest'\n",
            "Output:   '<extra_id_0> shuru karte hain.'\n",
            "\n",
            "Input:    'meri car kharab ho gayi hai, what should I do'\n",
            "Output:   '<extra_id_0> a query.'\n",
            "\n",
            "Input:    'yeh phone bahut expensive hai, is there any'\n",
            "Output:   '<extra_id_0> is query:'\n",
            "\n",
            "Input:    'kal ka match kiska hai? I want to see the'\n",
            "Output:   '<extra_id_0> is query:'\n",
            "\n",
            "Input:    'dosto, koi new movie aayi hai kya? I am so'\n",
            "Output:   '<extra_id_0>. I am sorry.'\n",
            "\n",
            "--- Running Zero-Shot for French-English ---\n",
            "Input:    'as good as vs as in'\n",
            "Output:   '<extra_id_0> as good as good'\n",
            "\n",
            "Input:    'did n't think covid-19'\n",
            "Output:   '<extra_id_0> n't think'\n",
            "\n",
            "Input:    'he is married to or with'\n",
            "Output:   '<extra_id_0> married to or with'\n",
            "\n",
            "--- Running Zero-Shot for Chinese-English ---\n",
            "Input:    'has got/have got 的中文翻译 have'\n",
            "Output:   '<extra_id_0>/have got 的中文'\n",
            "\n",
            "Input:    '有 has got/have got 的中文翻译'\n",
            "Output:   '<extra_id_0> 有 has got/have'\n",
            "\n",
            "Input:    '有 has got'\n",
            "Output:   '<extra_id_0> 有 has got'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Load the mT5 model and tokenizer.\n",
        "model_name = \"google/mt5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Define your zero-shot prompts and queries for each language pair.\n",
        "# A clear prompt helps the model understand the task.\n",
        "# Hinglish (Hindi-English)\n",
        "hinglish_queries = [\n",
        "    \"mujhe aaj bahar jana hai, can you please tell me the\",\n",
        "    \"kya aap mujhe bata sakte hain, what is the nearest\",\n",
        "    \"meri car kharab ho gayi hai, what should I do\",\n",
        "    \"yeh phone bahut expensive hai, is there any\",\n",
        "    \"kal ka match kiska hai? I want to see the\",\n",
        "    \"dosto, koi new movie aayi hai kya? I am so\",\n",
        "]\n",
        "\n",
        "# Frenglish (French-English)\n",
        "frenglish_queries = [\n",
        "    \"as good as vs as in\",\n",
        "    \"did n't think covid-19\",\n",
        "    \"he is married to or with\"\n",
        "]\n",
        "\n",
        "# Chinese-English\n",
        "chinese_queries = [\n",
        "    \"has got/have got 的中文翻译 have\",\n",
        "    \"有 has got/have got 的中文翻译\",\n",
        "    \"有 has got\"\n",
        "]\n",
        "\n",
        "\n",
        "# A function to run the zero-shot inference.\n",
        "def run_zero_shot_inference(queries, language_pair):\n",
        "    print(f\"--- Running Zero-Shot for {language_pair} ---\")\n",
        "\n",
        "    # Prepend a task-specific prefix with explicit language information.\n",
        "    prefix = f\"complete the query containing {language_pair.replace('-', ' and ')} code-mix words: \"\n",
        "\n",
        "    for query in queries:\n",
        "        input_text = prefix + query\n",
        "\n",
        "        # Tokenize the input text.\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "        # Generate the output.\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=50,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated tokens back to text.\n",
        "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"Input:    '{query}'\")\n",
        "        print(f\"Output:   '{generated_text}'\\n\")\n",
        "\n",
        "# Run the tests for each language pair.\n",
        "run_zero_shot_inference(hinglish_queries, \"Hindi-English\")\n",
        "run_zero_shot_inference(frenglish_queries, \"French-English\")\n",
        "run_zero_shot_inference(chinese_queries, \"Chinese-English\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results Summary\n",
        "\n",
        "The zero-shot evaluation with the mT5-base model, even with improved and more explicit prompts, yielded poor results across all three language pairs. The model consistently failed to provide meaningful auto-suggestions for the code-mixed queries. The outputs highlight a fundamental lack of understanding of the core task.\n",
        "\n",
        "* **Hinglish:** Despite the explicit prompt, the model's responses were often generic, returning placeholder tokens or phrases like `<extra_id_0> to find the answer.`\n",
        "* **French-English:** The outputs were similarly unhelpful, with the model failing to complete the queries in a natural, code-mixed way and often falling back to its default pre-training behavior.\n",
        "* **Chinese-English:** The model's responses were nonsensical, again dominated by the placeholder token `<extra_id_0>`, which indicates that the model could not generate a coherent continuation.\n",
        "\n",
        "This outcome confirms that a general multilingual model lacks the specialized knowledge of code-switching patterns necessary for this specific task. The poor baseline performance validates the project's central premise and provides strong evidence for the need to create and fine-tune a specialized model."
      ],
      "metadata": {
        "id": "mPbOCLILV21v"
      }
    }
  ]
}